{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, glob, subprocess\n",
    "from itertools import product\n",
    "\n",
    "import shutil\n",
    "#from IPython.display import Image as JImage\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(image):\n",
    "  \n",
    "    #cnn detector사용해서 얼굴 인식하는 것\n",
    "    cnn_face_detector=dlib.cnn_face_detection_model_v1(\"mmod_human_face_detector.dat\")\n",
    "    detected_faces=cnn_face_detector(img,1)\n",
    "    face_frames=[(x.rect.left(),x.rect.top(),x.rect.right(),x.rect.bottom()) for x in detected_faces]\n",
    "    \n",
    "    return face_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realtime(model_path): #실시간으로 딥러닝 모델 동작하는 함수 , 작동은 잘되는데, 너무 특정 각도에서만 동작\n",
    "    def label_img(img, label, loc=(3,50)):\n",
    "        return cv2.putText(img, label, loc, cv2.FONT_HERSHEY_SIMPLEX, 3.4, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "#cv2.putText는 웹캠 영상에다가 글쓰는거    \n",
    "    \n",
    "    \"\"\"     \n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# 얼굴인지 아닌지를 분류해줌 카스케이드 파일\n",
    "# 지금은 이 파일 대신 얼굴을 분류해주는 함수를 따로 만들어서 그걸 동작\n",
    "# 이 함수에서는 아직 안씀, 얼굴인지 분류 안해주고 일단 눈 떴는지 감았는지만\n",
    "    \"\"\"  \n",
    "    classes={1:\"open\",0:\"close\"} #classification 작업이기 때문에 0아니면 1\n",
    "    model=keras.models.load_model(model_path) #h5 가중치 파일 불러오기\n",
    "    \n",
    "    cap=cv2.VideoCapture(0) #웹캠으로부터 비디오 읽기 - 0이라는게 웹캠 카메라를 의미함\n",
    "    cap.open(0) #웹캠 열기\n",
    "    cap.set(3,640) #웹캠 사이즈 설정 - 3은 너비 \n",
    "    cap.set(4,480) #웹캠 사이즈 설정 - 4는 높이\n",
    "    \n",
    "    #noface=list()\n",
    "    while(True) : #인터럽트가 일어나야 종료가 되므로 그전까지 계속 웹캠 작동\n",
    "        images=list() #순간순간 캡처를 리스트로 저장\n",
    "        code,image = cap.read() #code는 capture결과 (boolean 타입), image는 capure한 frame\n",
    "        if code: #만약 capture 결과가 true라면 (즉 capture한게 있다면)\n",
    "            images.append(image) #리스트에 캡처한 이미지를 추가\n",
    "            #ace_rect=detect_faces(image)\n",
    "            #if len(face_rect)>0:\n",
    "                #mage=np.array(Image.fromarray(image).crop(face_rect[0]))\n",
    "            #lse:\n",
    "                #noface.append(i)\n",
    "            \n",
    "#우리는 데이터 224x224로 학습시켰기 때문에 영상도 224x224사이즈로 전처리     \n",
    "        processed_images=np.stack([cv2.resize(preprocess_input(img.astype(np.float32)),(224,224)) for img in images], axis=0)\n",
    "\n",
    "        \n",
    "        preds=model.predict(processed_images)\n",
    "        labels=[classes[p] for p in np.argmax(preds, axis=1)] #close/open label for each frame\n",
    "        #labels=[\"noface\" if i in noface else label for i,label in enumerate(labels)]\n",
    "        for j,image in enumerate(images):\n",
    "            which=0 if labels[j]==\"close\" else 1\n",
    "            label=labels[j] + \" (%0.2f)\" %(preds[j][which])\n",
    "            img_a=label_img(image, label) #annotate each original frame with predicted label\n",
    "            \n",
    "          #화면 제목 설정\n",
    "        cv2.imshow('frame',image)\n",
    "        \n",
    "        #q클릭하면 웹캠 정상 종료\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destoryAllWindows()\n",
    "    return\n",
    "    \n",
    "    \n",
    "model_path=\"FINAL1.h5\" # 모델경로\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nays1\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\nays1\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nays1\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.cv2' has no attribute 'destoryAllWindows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c0fe8a845671>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrealtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-e163823092d8>\u001b[0m in \u001b[0;36mrealtime\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestoryAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2.cv2' has no attribute 'destoryAllWindows'"
     ]
    }
   ],
   "source": [
    "realtime(model_path) #위에서 정의한 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(image):\n",
    "  #이거는 dlib라이브러리 사용해서 얼굴임을 인식하는 함수\n",
    "    face_detector=dlib.get_frontal_face_detector()\n",
    "    detected_faces=face_detector(image,1)\n",
    "    face_frames=[(x.left(),x.top(),x.right(),x.bottom()) for x in detected_faces]\n",
    "    \n",
    "   \n",
    "    \n",
    "    return face_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realtime2(model_path): #이건 실시간으로 얼굴인식 + 딥러닝으로 눈 구별 하려고하는 함순데 구현 못하겠음\n",
    "    def label_img(img, label, loc=(3,50)): #annotates the image with the predicted label (close/open)\n",
    "        return cv2.putText(img, label, loc, cv2.FONT_HERSHEY_SIMPLEX, 3.4, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cap=cv2.VideoCapture(0)\n",
    "    cap.set(3,640)\n",
    "    cap.set(4,480)\n",
    "    \n",
    "    \n",
    "    \n",
    "    classes={1:\"open\",0:\"close\"} #classification 작업이기 때문에 0아니면 1\n",
    "    model=keras.models.load_model(model_path) #h5 가중치 파일 불러오기\n",
    "    \n",
    "    \n",
    "    noface=list()\n",
    "    while(True) : #인터럽트가 일어나야 종료가 되므로 그전까지 계속 웹캠 작동\n",
    "        images=list() #순간순간 캡처를 리스트로 저장\n",
    "        code,image = cap.read() #code는 capture결과 (boolean 타입), image는 capure한 frame\n",
    "        if code: #만약 capture 결과가 true라면 (즉 capture한게 있다면)\n",
    "            \n",
    "            \n",
    "            face_rect=detect_faces(image)# detect_faces함수에 캡처한 이미지를 매개변수로\n",
    "            if len(face_rect)>0: #face가 하나라도 있으면\n",
    "                image=np.array(Image.fromarray(image).crop(face_rect[0]))\n",
    "            else:\n",
    "                noface.append(cap.get(cv2.CAP_PROP_FPS))\n",
    "                \n",
    "            images.append(image) #리스트에 캡처한 이미지를 추가\n",
    "            \n",
    "            \n",
    "#우리는 데이터 224x224로 학습시켰기 때문에 영상도 224x224사이즈로 전처리     \n",
    "        processed_images=np.stack([cv2.resize(preprocess_input(img.astype(np.float32)),(224,224)) for img in images], axis=0)\n",
    "\n",
    "        \n",
    "        preds=model.predict(processed_images)\n",
    "        labels=[classes[p] for p in np.argmax(preds, axis=1)] #close/open label for each frame\n",
    "        labels=[\"noface\" if i in noface else label for i,label in enumerate(labels)]\n",
    "        for j,image in enumerate(images):\n",
    "            which=0 if labels[j]==\"close\" else 1\n",
    "            label=labels[j] + (\" (%0.2f)\" %(preds[j][which]) if label in classes.keys() else '')\n",
    "            img_a=label_img(image, label) #annotate each original frame with predicted label\n",
    "            \n",
    "          #화면 제목 설정\n",
    "        cv2.imshow('frame',image)\n",
    "        \n",
    "        #q클릭하면 웹캠 정상 종료\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    return;\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'label' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-1a2ba7120ee7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrealtime2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-83e18dbb2828>\u001b[0m in \u001b[0;36mrealtime2\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mwhich\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"close\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" (%0.2f)\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwhich\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mimg_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#annotate each original frame with predicted label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'label' referenced before assignment"
     ]
    }
   ],
   "source": [
    "realtime2(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
