{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, glob, subprocess\n",
    "from itertools import product\n",
    "\n",
    "import shutil\n",
    "#from IPython.display import Image as JImage\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(image):\n",
    "  \n",
    "    #cnn detector사용해서 얼굴 인식하는 것\n",
    "    cnn_face_detector=dlib.cnn_face_detection_model_v1(\"mmod_human_face_detector.dat\")\n",
    "    detected_faces=cnn_face_detector(img,1)\n",
    "    face_frames=[(x.rect.left(),x.rect.top(),x.rect.right(),x.rect.bottom()) for x in detected_faces]\n",
    "    \n",
    "    return face_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realtime(model_path): #실시간으로 딥러닝 모델 동작하는 함수 , 작동은 잘되는데, 너무 특정 각도에서만 동작\n",
    "    def label_img(img, label, loc=(3,50)):\n",
    "        return cv2.putText(img, label, loc, cv2.FONT_HERSHEY_SIMPLEX, 3.4, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "#cv2.putText는 웹캠 영상에다가 글쓰는거    \n",
    "    \n",
    "    \"\"\"     \n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# 얼굴인지 아닌지를 분류해줌 카스케이드 파일\n",
    "# 지금은 이 파일 대신 얼굴을 분류해주는 함수를 따로 만들어서 그걸 동작\n",
    "# 이 함수에서는 아직 안씀, 얼굴인지 분류 안해주고 일단 눈 떴는지 감았는지만\n",
    "    \"\"\"  \n",
    "    classes={1:\"open\",0:\"close\"} #classification 작업이기 때문에 0아니면 1\n",
    "    model=keras.models.load_model(model_path) #h5 가중치 파일 불러오기\n",
    "    \n",
    "    cap=cv2.VideoCapture(0) #웹캠으로부터 비디오 읽기 - 0이라는게 웹캠 카메라를 의미함\n",
    "    cap.open(0) #웹캠 열기\n",
    "    cap.set(3,640) #웹캠 사이즈 설정 - 3은 너비 \n",
    "    cap.set(4,480) #웹캠 사이즈 설정 - 4는 높이\n",
    "    \n",
    "    #noface=list()\n",
    "    while(True) : #인터럽트가 일어나야 종료가 되므로 그전까지 계속 웹캠 작동\n",
    "        images=list() #순간순간 캡처를 리스트로 저장\n",
    "        code,image = cap.read() #code는 capture결과 (boolean 타입), image는 capure한 frame\n",
    "        if code: #만약 capture 결과가 true라면 (즉 capture한게 있다면)\n",
    "            images.append(image) #리스트에 캡처한 이미지를 추가\n",
    "            #ace_rect=detect_faces(image)\n",
    "            #if len(face_rect)>0:\n",
    "                #mage=np.array(Image.fromarray(image).crop(face_rect[0]))\n",
    "            #lse:\n",
    "                #noface.append(i)\n",
    "            \n",
    "#우리는 데이터 224x224로 학습시켰기 때문에 영상도 224x224사이즈로 전처리     \n",
    "        processed_images=np.stack([cv2.resize(preprocess_input(img.astype(np.float32)),(224,224)) for img in images], axis=0)\n",
    "\n",
    "        \n",
    "        preds=model.predict(processed_images)\n",
    "        labels=[classes[p] for p in np.argmax(preds, axis=1)] #close/open label for each frame\n",
    "        #labels=[\"noface\" if i in noface else label for i,label in enumerate(labels)]\n",
    "        for j,image in enumerate(images):\n",
    "            which=0 if labels[j]==\"close\" else 1\n",
    "            label=labels[j] + \" (%0.2f)\" %(preds[j][which])\n",
    "            img_a=label_img(image, label) #annotate each original frame with predicted label\n",
    "            \n",
    "          #화면 제목 설정\n",
    "        cv2.imshow('frame',image)\n",
    "        \n",
    "        #q클릭하면 웹캠 정상 종료\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destoryAllWindows()\n",
    "    return\n",
    "    \n",
    "    \n",
    "model_path=\"FINAL1.h5\" # 모델경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\dlib_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\dlib_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\dlib_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-33dae2ccf645>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrealtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#위에서 정의한 함수 실행\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-21bd68b5da7e>\u001b[0m in \u001b[0;36mrealtime\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m#우리는 데이터 224x224로 학습시켰기 때문에 영상도 224x224사이즈로 전처리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mprocessed_images\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dlib_env\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'need at least one array to stack'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "realtime(model_path) #위에서 정의한 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
